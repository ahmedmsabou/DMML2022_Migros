{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Free Attempt for the DMML Project, Ahmed and Joris\n",
        "Inspired by: https://www.kaggle.com/code/houssemayed/camembert-for-french-tweets-classification/notebook\n",
        "\n",
        "We will use CamemBERT to do the classification. Note: You will need to use a GPU on colab to run this computationally heavy model!\n",
        "\n",
        "First, load packages and data"
      ],
      "metadata": {
        "id": "hi8INGBJXJ-G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiWkga-C2AhP",
        "outputId": "8cf0ab08-484e-46b9-9083-5d4c815504da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 33.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 61.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 76.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 25.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "\n",
        "# read data and import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "!pip install -U spacy\n",
        "\n",
        "# import some additional packages\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn. preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from transformers import DataCollatorWithPadding,AutoModelForSequenceClassification, Trainer, TrainingArguments,AutoTokenizer,AutoModel,AutoConfig\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# train data\n",
        "df_train = pd.read_csv('training_data.csv')\n",
        "\n",
        "# Rename Labeling\n",
        "df_train['difficulty'] = df_train['difficulty'].replace(['A1','A2','B1', 'B2', 'C1', 'C2'],[0,1,2,3,4,5])\n",
        "\n",
        "# test data\n",
        "df_test = pd.read_csv('unlabelled_test_data.csv')\n",
        "\n",
        "\n",
        "# Imports to use a pipeline\n",
        "from tqdm import tqdm, trange\n",
        "from tensorflow import keras \n",
        "\n",
        "# Imports for modelling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate(true, pred):\n",
        "    precision = precision_score(true, pred, average = 'weighted')\n",
        "    recall = recall_score(true, pred, average = 'weighted')\n",
        "    f1 = f1_score(true, pred, average = 'weighted')\n",
        "    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(true, pred)}\")\n",
        "    print(f\"ACCURACY SCORE:\\n{accuracy_score(true, pred):.4f}\")\n",
        "    print(f\"CLASSIFICATION REPORT:\\n\\tPrecision: {precision:.4f}\\n\\tRecall: {recall:.4f}\\n\\tF1_Score: {f1:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Parameters, load pretrained Tokenizer"
      ],
      "metadata": {
        "id": "RHt-LckWf51S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These parameters are not tuned, but we checked a few possiblities to get close to the ideal value\n",
        "epochs = 5\n",
        "MAX_LEN = 64\n",
        "batch_size = 16\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Here we load the CamemBERT tokenizer and use lower_case\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert/camembert-large', do_lower_case=True)\n",
        "\n",
        "# As list is required as input\n",
        "text = df_train['sentence'].to_list()\n",
        "labels = df_train['difficulty'].to_list()\n"
      ],
      "metadata": {
        "id": "Y6ldOvP5f5hD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Input for the CamemBERT model\n"
      ],
      "metadata": {
        "id": "uf1_FkpqgKe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CamamBERT tokenizer is used to transform input_ids'\n",
        "input_ids  = [tokenizer.encode(sent,add_special_tokens=True,max_length=MAX_LEN) for sent in text]\n",
        "\n",
        "# the input tokens are padded\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "attention_masks = []\n",
        "\n",
        "# Use 0s for padding and creating a mask of 1s\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]  \n",
        "    attention_masks.append(seq_mask)\n",
        "\n",
        "\n",
        "# Split 80/20 Train/Validation\n",
        "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,\n",
        "                                                            random_state=42, test_size=0.2)\n",
        "\n",
        "\n",
        "# torch tensors is the required datatype for the BERT model, so change input\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "# Create an iterator, as it will use less memory than a loop\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "pZs59TtigK9f"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and prepare CamemBERT model\n",
        "\n"
      ],
      "metadata": {
        "id": "KQ4xoueKiVZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and set it to device\n",
        "model = CamembertForSequenceClassification.from_pretrained(\"camembert/camembert-large\", num_labels=6)\n",
        "model.to(device)\n",
        "\n",
        "# Parameter settings\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "# Use AdamW as optimizer\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5, eps=1e-8)\n",
        "\n",
        "# Calculate the accuracy, simply assign the text difficulty to the one with the highest probability (argmax)\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# Linear scheduler for warmup\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(train_labels)*epochs)\n",
        "\n",
        "# Store loss\n",
        "train_loss_set = []\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNxO18DeiV7f",
        "outputId": "b9b6b5d1-2665-4bde-e945-6e3a3206f919"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at camembert/camembert-large were not used when initializing CamembertForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert/camembert-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train already pre-trained model"
      ],
      "metadata": {
        "id": "jO3XP7Eyid2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for loop over each epoch (5)\n",
        "for _ in trange(epochs, desc=\"Epoch\"):  \n",
        "\n",
        "    # Keep track of variables during the training\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  \n",
        "    # Starting trained the pre-trained BERT model on our data set\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Assign batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Optain inputs from the previously made dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "        # Obtain loss value\n",
        "        loss = outputs[0]\n",
        "        # Append to training loss set\n",
        "        train_loss_set.append(loss.item())    \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        # Step optimizer (gradient) and scheduler\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Update variables that track results\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    \n",
        "    \n",
        "\n",
        "    # Check the results on the validation set\n",
        "    # Tracking variables\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    # Put model in validation mode\n",
        "    model.eval()\n",
        "    # Evaluation results for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        # Batch assigned to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # unpack inputs\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # don't compute/store gradients\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, and calculate predictions\n",
        "            outputs =  model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "            loss, logits = outputs[:2]\n",
        "    \n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Print resulting validation accuracy\n",
        "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "\n",
        "# Note, the validation accuracy changes every time you re-run it"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neX8ZjgdieV7",
        "outputId": "bfcd093e-8b39-4a29-e83a-1d228bb22e2a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 1.4416452964146933\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch:  20%|██        | 1/5 [02:44<10:56, 164.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4239583333333333\n",
            "Train loss: 1.0896249944965044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  40%|████      | 2/5 [05:27<08:11, 163.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.49375\n",
            "Train loss: 0.9461640218893687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  60%|██████    | 3/5 [08:10<05:27, 163.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5645833333333333\n",
            "Train loss: 0.8505047939717769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  80%|████████  | 4/5 [10:54<02:43, 163.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5760416666666667\n",
            "Train loss: 0.7996513730535905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 5/5 [13:37<00:00, 163.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.528125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check performance on Validation set"
      ],
      "metadata": {
        "id": "L69sMMHzkcW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on validation set\n",
        "eval_pred = []\n",
        "with torch.no_grad():\n",
        "    # Forward pass, calculate logit predictions\n",
        "    outputs =  model(validation_inputs.to(device),token_type_ids=None, attention_mask=validation_masks.to(device))\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy() \n",
        "    eval_pred.extend(np.argmax(logits, axis=1).flatten())\n",
        "\n",
        "evaluate(validation_labels, eval_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzStBM6YQJOA",
        "outputId": "637fd446-c857-4aaf-88bf-3131da33df2b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONFUSION MATRIX:\n",
            "[[ 90  73   3   0   0   0]\n",
            " [ 14 115  28   0   1   0]\n",
            " [  8  68  83   4   1   2]\n",
            " [  0   5  52  58  32   6]\n",
            " [  0   1  14  36  88  13]\n",
            " [  0   1   9  20  61  74]]\n",
            "ACCURACY SCORE:\n",
            "0.5292\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.5752\n",
            "\tRecall: 0.5292\n",
            "\tF1_Score: 0.5320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# After Training the model, make predictions\n",
        "\n",
        "Use the format from the sample submission."
      ],
      "metadata": {
        "id": "jp2lPdQVFD-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments = df_test['sentence'].to_list()\n",
        "\n",
        "# Encode the comments\n",
        "tokenized_comments_ids = [tokenizer.encode(comment,add_special_tokens=True,max_length=MAX_LEN) for comment in comments]\n",
        "# Pad the resulted encoded comments\n",
        "tokenized_comments_ids = pad_sequences(tokenized_comments_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks \n",
        "attention_masks = []\n",
        "for seq in tokenized_comments_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "\n",
        "prediction_inputs = torch.tensor(tokenized_comments_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "\n",
        "# Apply the finetuned model (Camembert)\n",
        "flat_pred = []\n",
        "with torch.no_grad():\n",
        "    # Forward pass, calculate logit predictions\n",
        "    outputs =  model(prediction_inputs.to(device),token_type_ids=None, attention_mask=prediction_masks.to(device))\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy() \n",
        "    flat_pred.extend(np.argmax(logits, axis=1).flatten())\n",
        "\n",
        "df_samplesub = pd.read_csv('sample_submission.csv')\n",
        "df_samplesub['difficulty'] = flat_pred\n",
        "\n",
        "# Translate back\n",
        "df_samplesub['difficulty'] = df_samplesub['difficulty'].replace([0,1,2,3,4,5],['A1','A2','B1', 'B2', 'C1', 'C2'])\n",
        "\n",
        "# Upload pandas dataframe as csv to drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('drive')\n",
        "\n",
        "# df_samplesub.to_csv('FinalTest4.csv', index = False)\n",
        "# !cp FinalTest4.csv \"drive/My Drive/\"\n"
      ],
      "metadata": {
        "id": "pOv99pWAFFL5"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}